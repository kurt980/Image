{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x2121fc0e460>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image name: train_mini/02989_Animalia_Chordata_Amphibia_Anura_Hylidae_Hyla_gratiosa/c07e2e0a-9eb3-4af5-b4dd-783493d7182e.jpg\n",
      "Amphibia Barking Tree Frog\n"
     ]
    }
   ],
   "source": [
    "mbra_df = pd.read_csv('mbra_dataset.csv')\n",
    "\n",
    "n = 3833\n",
    "img_name = mbra_df.iloc[n, 0]\n",
    "class_name = mbra_df.loc[n, 'class']\n",
    "common_name = mbra_df.loc[n, 'common_name']\n",
    "\n",
    "print('Image name: {}'.format(img_name))\n",
    "print(class_name, common_name)\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 8\n",
    "num_classes = 4\n",
    "batch_size = 256\n",
    "learning_rate = 0.002\n",
    "\n",
    "root_path = 'C:/Users/KurtJi/OneDrive - University of Illinois - Urbana/Desktop/Personal Projects/data/inaturalist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': array([[[247, 200, 156],\n",
       "         [144, 100,  55],\n",
       "         [ 99,  54,  15],\n",
       "         ...,\n",
       "         [ 44,  18,   5],\n",
       "         [ 76,  50,  37],\n",
       "         [ 86,  60,  47]],\n",
       " \n",
       "        [[251, 208, 166],\n",
       "         [202, 160, 112],\n",
       "         [163, 119,  70],\n",
       "         ...,\n",
       "         [ 66,  47,  30],\n",
       "         [ 84,  65,  48],\n",
       "         [ 78,  59,  42]],\n",
       " \n",
       "        [[240, 201, 168],\n",
       "         [175, 137,  90],\n",
       "         [221, 179, 119],\n",
       "         ...,\n",
       "         [ 83,  70,  53],\n",
       "         [ 66,  53,  36],\n",
       "         [ 49,  36,  19]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[255, 239, 213],\n",
       "         [149, 121,  97],\n",
       "         [ 51,  25,   2],\n",
       "         ...,\n",
       "         [255, 244, 251],\n",
       "         [185, 156, 150],\n",
       "         [122,  91,  73]],\n",
       " \n",
       "        [[208, 179, 163],\n",
       "         [ 86,  59,  42],\n",
       "         [ 52,  27,   7],\n",
       "         ...,\n",
       "         [174, 158, 169],\n",
       "         [255, 247, 249],\n",
       "         [171, 152, 146]],\n",
       " \n",
       "        [[159, 130, 114],\n",
       "         [ 77,  50,  33],\n",
       "         [ 51,  26,   6],\n",
       "         ...,\n",
       "         [ 75,  70,  74],\n",
       "         [214, 206, 203],\n",
       "         [240, 231, 224]]], dtype=uint8),\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### **Dataset Class**\n",
    "class MbraDataset(Dataset):\n",
    "    \"\"\"Dataset class for the MBRA dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame with image_path and class_id.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        class_id = int(self.df.iloc[idx, -1])  # selecting the last column as class_id\n",
    "        sample = {'image': image, 'label': class_id}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
    "        image = io.imread(img_path) # Load the image using skimage.io\n",
    "        label = self.df.iloc[idx, 1]\n",
    "\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Convert class names to unique integer identifiers\n",
    "label_encoder = LabelEncoder()\n",
    "mbra_df['class_id'] = label_encoder.fit_transform(mbra_df['class'])\n",
    "\n",
    "# Check the transformation\n",
    "mbra_df[['class', 'class_id']].head()\n",
    "\n",
    "# Create a new DataFrame with only necessary columns\n",
    "mbra_df_class = mbra_df[['image_path', 'class_id']].copy()\n",
    "\n",
    "# Test the dataset class\n",
    "mbra_dataset = MbraDataset(df=mbra_df_class, root_dir=root_path)\n",
    "\n",
    "# Get the first sample to test the dataset class\n",
    "first_sample = mbra_dataset[0] if len(mbra_dataset) > 0 else None\n",
    "first_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([[[[0.0566, 0.0640, 0.0664,  ..., 0.3324, 0.2555, 0.2578],\n",
      "          [0.0501, 0.0623, 0.0643,  ..., 0.2690, 0.2389, 0.2264],\n",
      "          [0.0520, 0.0629, 0.0613,  ..., 0.2794, 0.2524, 0.2775],\n",
      "          ...,\n",
      "          [0.1254, 0.1253, 0.1256,  ..., 0.2586, 0.2208, 0.1852],\n",
      "          [0.1206, 0.1194, 0.1244,  ..., 0.2410, 0.3343, 0.3436],\n",
      "          [0.1214, 0.1175, 0.1180,  ..., 0.2064, 0.2569, 0.3446]],\n",
      "\n",
      "         [[0.0949, 0.0993, 0.1017,  ..., 0.3856, 0.2989, 0.2923],\n",
      "          [0.0893, 0.0976, 0.0995,  ..., 0.3213, 0.2872, 0.2692],\n",
      "          [0.0912, 0.0966, 0.0929,  ..., 0.3347, 0.3019, 0.3246],\n",
      "          ...,\n",
      "          [0.1332, 0.1332, 0.1334,  ..., 0.2541, 0.2160, 0.1799],\n",
      "          [0.1284, 0.1273, 0.1322,  ..., 0.2275, 0.3153, 0.3177],\n",
      "          [0.1293, 0.1254, 0.1259,  ..., 0.1976, 0.2382, 0.3206]],\n",
      "\n",
      "         [[0.1302, 0.1346, 0.1370,  ..., 0.2994, 0.2333, 0.2385],\n",
      "          [0.1263, 0.1358, 0.1351,  ..., 0.2446, 0.2217, 0.2134],\n",
      "          [0.1304, 0.1386, 0.1356,  ..., 0.2668, 0.2465, 0.2728],\n",
      "          ...,\n",
      "          [0.1214, 0.1214, 0.1217,  ..., 0.2466, 0.2078, 0.1717],\n",
      "          [0.1166, 0.1155, 0.1217,  ..., 0.2197, 0.2987, 0.2949],\n",
      "          [0.1175, 0.1136, 0.1154,  ..., 0.1628, 0.1964, 0.2686]]],\n",
      "\n",
      "\n",
      "        [[[0.0296, 0.0616, 0.0901,  ..., 0.0367, 0.0374, 0.0374],\n",
      "          [0.0233, 0.0303, 0.0516,  ..., 0.0319, 0.0280, 0.0294],\n",
      "          [0.0296, 0.0227, 0.0260,  ..., 0.0314, 0.0310, 0.0282],\n",
      "          ...,\n",
      "          [0.8117, 0.8186, 0.8248,  ..., 0.4510, 0.4438, 0.4326],\n",
      "          [0.8207, 0.8305, 0.8297,  ..., 0.4358, 0.4345, 0.4352],\n",
      "          [0.8232, 0.8419, 0.8377,  ..., 0.4213, 0.4370, 0.4406]],\n",
      "\n",
      "         [[0.0930, 0.1306, 0.1721,  ..., 0.0406, 0.0413, 0.0413],\n",
      "          [0.0785, 0.0883, 0.1162,  ..., 0.0358, 0.0319, 0.0333],\n",
      "          [0.0767, 0.0711, 0.0751,  ..., 0.0353, 0.0349, 0.0322],\n",
      "          ...,\n",
      "          [0.5138, 0.5197, 0.5343,  ..., 0.2814, 0.2647, 0.2535],\n",
      "          [0.5259, 0.5187, 0.5345,  ..., 0.2843, 0.2767, 0.2774],\n",
      "          [0.5294, 0.5306, 0.5434,  ..., 0.2765, 0.2840, 0.2837]],\n",
      "\n",
      "         [[0.0185, 0.0548, 0.0910,  ..., 0.0488, 0.0570, 0.0571],\n",
      "          [0.0059, 0.0144, 0.0414,  ..., 0.0511, 0.0483, 0.0523],\n",
      "          [0.0081, 0.0029, 0.0094,  ..., 0.0522, 0.0542, 0.0516],\n",
      "          ...,\n",
      "          [0.5132, 0.5200, 0.5380,  ..., 0.1670, 0.1628, 0.1550],\n",
      "          [0.5158, 0.5195, 0.5352,  ..., 0.1688, 0.1759, 0.1795],\n",
      "          [0.5185, 0.5309, 0.5434,  ..., 0.1609, 0.1819, 0.1857]]],\n",
      "\n",
      "\n",
      "        [[[0.4969, 0.4787, 0.4562,  ..., 0.1242, 0.0982, 0.1317],\n",
      "          [0.5032, 0.4906, 0.4699,  ..., 0.0600, 0.2337, 0.4274],\n",
      "          [0.5111, 0.4842, 0.4718,  ..., 0.3225, 0.3893, 0.3486],\n",
      "          ...,\n",
      "          [0.5273, 0.5270, 0.5181,  ..., 0.3519, 0.3513, 0.3659],\n",
      "          [0.5244, 0.5248, 0.5174,  ..., 0.3060, 0.3338, 0.3431],\n",
      "          [0.4932, 0.4999, 0.5002,  ..., 0.3258, 0.3522, 0.3511]],\n",
      "\n",
      "         [[0.5936, 0.5882, 0.5702,  ..., 0.1345, 0.1196, 0.1593],\n",
      "          [0.6090, 0.6019, 0.5808,  ..., 0.0712, 0.2890, 0.4903],\n",
      "          [0.6171, 0.5967, 0.5903,  ..., 0.4409, 0.4929, 0.4557],\n",
      "          ...,\n",
      "          [0.6093, 0.6083, 0.6040,  ..., 0.4270, 0.3938, 0.4086],\n",
      "          [0.6094, 0.6091, 0.6044,  ..., 0.3745, 0.4041, 0.4122],\n",
      "          [0.5890, 0.5888, 0.5900,  ..., 0.3912, 0.4190, 0.4207]],\n",
      "\n",
      "         [[0.4569, 0.3984, 0.3937,  ..., 0.0293, 0.0540, 0.0936],\n",
      "          [0.4384, 0.4088, 0.4168,  ..., 0.0224, 0.2019, 0.3992],\n",
      "          [0.4536, 0.4162, 0.4096,  ..., 0.3158, 0.3647, 0.3258],\n",
      "          ...,\n",
      "          [0.4688, 0.4630, 0.4244,  ..., 0.2107, 0.2084, 0.2160],\n",
      "          [0.4739, 0.4682, 0.4310,  ..., 0.2070, 0.2520, 0.2434],\n",
      "          [0.4424, 0.4437, 0.4225,  ..., 0.2072, 0.2299, 0.2183]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.3439, 0.3436, 0.3449,  ..., 0.4410, 0.4401, 0.4376],\n",
      "          [0.3200, 0.3375, 0.3435,  ..., 0.4891, 0.4826, 0.4950],\n",
      "          [0.3257, 0.3507, 0.3685,  ..., 0.4518, 0.4521, 0.4653],\n",
      "          ...,\n",
      "          [0.5832, 0.4477, 0.2846,  ..., 0.2354, 0.2390, 0.2506],\n",
      "          [0.4484, 0.3558, 0.2256,  ..., 0.2328, 0.2569, 0.2789],\n",
      "          [0.1806, 0.2296, 0.2556,  ..., 0.2467, 0.2685, 0.2925]],\n",
      "\n",
      "         [[0.4594, 0.4695, 0.4863,  ..., 0.5413, 0.5435, 0.5443],\n",
      "          [0.4513, 0.4775, 0.4973,  ..., 0.5968, 0.6008, 0.6252],\n",
      "          [0.4590, 0.4911, 0.5163,  ..., 0.5701, 0.5738, 0.5866],\n",
      "          ...,\n",
      "          [0.5934, 0.4588, 0.2950,  ..., 0.4481, 0.4547, 0.4716],\n",
      "          [0.4648, 0.3694, 0.2411,  ..., 0.4705, 0.4774, 0.4759],\n",
      "          [0.1938, 0.2414, 0.2713,  ..., 0.4709, 0.4767, 0.4751]],\n",
      "\n",
      "         [[0.2807, 0.2777, 0.2734,  ..., 0.4136, 0.3937, 0.3651],\n",
      "          [0.2433, 0.2517, 0.2505,  ..., 0.4193, 0.3607, 0.3113],\n",
      "          [0.2437, 0.2603, 0.2671,  ..., 0.3377, 0.3402, 0.3543],\n",
      "          ...,\n",
      "          [0.6699, 0.5307, 0.3498,  ..., 0.1053, 0.1062, 0.1203],\n",
      "          [0.5243, 0.4331, 0.2851,  ..., 0.0692, 0.0982, 0.1449],\n",
      "          [0.2483, 0.3003, 0.3146,  ..., 0.0802, 0.1074, 0.1585]]],\n",
      "\n",
      "\n",
      "        [[[0.1799, 0.2069, 0.1728,  ..., 0.2007, 0.1823, 0.1716],\n",
      "          [0.1774, 0.1937, 0.1848,  ..., 0.1991, 0.1858, 0.1783],\n",
      "          [0.1677, 0.1817, 0.1556,  ..., 0.2008, 0.1899, 0.1812],\n",
      "          ...,\n",
      "          [0.0405, 0.0460, 0.0488,  ..., 0.5391, 0.5399, 0.5889],\n",
      "          [0.0514, 0.0558, 0.0570,  ..., 0.5060, 0.5295, 0.5505],\n",
      "          [0.0548, 0.0583, 0.0588,  ..., 0.4458, 0.4536, 0.4806]],\n",
      "\n",
      "         [[0.1132, 0.1421, 0.1166,  ..., 0.1777, 0.1701, 0.1598],\n",
      "          [0.1091, 0.1298, 0.1298,  ..., 0.1751, 0.1685, 0.1613],\n",
      "          [0.1031, 0.1229, 0.1057,  ..., 0.1741, 0.1662, 0.1577],\n",
      "          ...,\n",
      "          [0.0179, 0.0234, 0.0263,  ..., 0.5604, 0.5919, 0.6503],\n",
      "          [0.0200, 0.0244, 0.0256,  ..., 0.5271, 0.5867, 0.6093],\n",
      "          [0.0234, 0.0269, 0.0274,  ..., 0.4744, 0.5175, 0.5461]],\n",
      "\n",
      "         [[0.0505, 0.0790, 0.0442,  ..., 0.1151, 0.1111, 0.1010],\n",
      "          [0.0498, 0.0688, 0.0646,  ..., 0.1127, 0.1112, 0.1042],\n",
      "          [0.0508, 0.0630, 0.0420,  ..., 0.1116, 0.1110, 0.1028],\n",
      "          ...,\n",
      "          [0.0032, 0.0087, 0.0116,  ..., 0.3070, 0.3192, 0.3740],\n",
      "          [0.0082, 0.0126, 0.0138,  ..., 0.2758, 0.3171, 0.3387],\n",
      "          [0.0116, 0.0152, 0.0157,  ..., 0.2060, 0.2440, 0.2722]]],\n",
      "\n",
      "\n",
      "        [[[0.0074, 0.1600, 0.1912,  ..., 0.2741, 0.1600, 0.1566],\n",
      "          [0.0245, 0.0548, 0.1022,  ..., 0.2742, 0.2142, 0.1893],\n",
      "          [0.0620, 0.0702, 0.0969,  ..., 0.2748, 0.2023, 0.1610],\n",
      "          ...,\n",
      "          [0.1995, 0.3174, 0.3806,  ..., 0.4572, 0.3864, 0.3035],\n",
      "          [0.1432, 0.3450, 0.3731,  ..., 0.3949, 0.3634, 0.2369],\n",
      "          [0.1007, 0.2709, 0.3564,  ..., 0.3842, 0.3521, 0.2794]],\n",
      "\n",
      "         [[0.0691, 0.2330, 0.3000,  ..., 0.4636, 0.3238, 0.2373],\n",
      "          [0.0780, 0.0982, 0.2181,  ..., 0.4780, 0.4358, 0.3425],\n",
      "          [0.1008, 0.0835, 0.1894,  ..., 0.4641, 0.4367, 0.3464],\n",
      "          ...,\n",
      "          [0.1577, 0.2416, 0.3238,  ..., 0.4698, 0.4315, 0.2560],\n",
      "          [0.1069, 0.2751, 0.3212,  ..., 0.3857, 0.4396, 0.2017],\n",
      "          [0.0656, 0.2037, 0.3059,  ..., 0.3536, 0.4706, 0.3010]],\n",
      "\n",
      "         [[0.0057, 0.1021, 0.0699,  ..., 0.2804, 0.1642, 0.0658],\n",
      "          [0.0341, 0.0333, 0.0399,  ..., 0.3397, 0.2749, 0.1826],\n",
      "          [0.0768, 0.0726, 0.0681,  ..., 0.3282, 0.3035, 0.2135],\n",
      "          ...,\n",
      "          [0.1272, 0.1493, 0.2000,  ..., 0.2251, 0.0697, 0.0645],\n",
      "          [0.0801, 0.1818, 0.2021,  ..., 0.1745, 0.1632, 0.0788],\n",
      "          [0.0443, 0.1100, 0.1938,  ..., 0.1748, 0.2378, 0.1785]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x238144 and 1600x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18808\\3331368115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\KurtJi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\KurtJi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18808\\3331368115.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\KurtJi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\KurtJi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\KurtJi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x238144 and 1600x128)"
     ]
    }
   ],
   "source": [
    "device = torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(device)\n",
    "### **Model Structure**\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "# Creating a CNN class\n",
    "# image size is (3,256, 256)\n",
    "class SimpleCNN(nn.Module):\n",
    "\t#  Determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "                \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "model = SimpleCNN(num_classes).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Add Transformer**\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        # if isinstance(self.output_size, int):\n",
    "        #     if h > w:\n",
    "        #         new_h, new_w = self.output_size * h / w, self.output_size\n",
    "        #     else:\n",
    "        #         new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        # else:\n",
    "        new_h, new_w = self.output_size, self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for label because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        # label = label * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'label': label}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h + 1)\n",
    "        left = np.random.randint(0, w - new_w + 1)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        # label = label - [left, top]\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image).float(),  # convert the image to a float Tensor\n",
    "                'label': label}  # keep the label as an integer\n",
    "    \n",
    "mbra_data_class = MbraDataset(df = mbra_df_class,\n",
    "                                           root_dir=root_path,\n",
    "                                           transform=transforms.Compose([\n",
    "                                               Rescale(256),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(mbra_data_class))\n",
    "test_size = len(mbra_data_class) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(mbra_data_class, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=256//2,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "len(train_loader)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "print(total_step)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:  # replace 'data_loader' with your actual DataLoader\n",
    "        images = batch['image']\n",
    "        labels = batch['label']\n",
    "        print(type(images), type(labels))\n",
    "        images = images.to(device)\n",
    "        print(images)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
